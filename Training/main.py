import pickle
import networkx as nx
import numpy as np
# import matplotlib.pyplot as plt
import pandas as pd
import gc #garbage collection
import torch
import torch.nn.functional as F
import torch.nn as nn
from torch_geometric.nn import GCNConv, SAGEConv, GraphConv
from torch_geometric.utils.convert import from_networkx

from sklearn import preprocessing


def normalize_data(data):
    data = data.to_numpy()
    # TODO: loai 50% so canh
    # median = np.median(data)
    # data[data <= median] = 0
    # data = data / (np.amax(data, axis=1) + 1e-9)
    data = data / np.amax(data, axis=1)
    return np.tril(data)


def load_data_2_create_graph(data_path):
    adj_matr = pd.read_csv(data_path, index_col=None, header=None)
    print(adj_matr.shape)
    adj_matr = normalize_data(adj_matr)
    print(adj_matr)
    G = nx.Graph(adj_matr)
    del adj_matr
    gc.collect()
    return G


class GCN(torch.nn.Module):
    def __init__(self):
        super().__init__()
        # self.conv1 = SAGEConv(-1, 64, normalize=True)
        # self.conv2 = SAGEConv(-1, dataset.num_classes, normalize=True)
        self.conv1 = GraphConv(-1, 32, aggr='mean', normalize=True)
        self.conv2 = GraphConv(-1, 5, aggr='mean', normalize=True)

    def forward(self, data):
        x, edge_index, weight = data.x, data.edge_index, data.weight
        x = self.conv1(x, edge_index, weight)
        x = F.relu(x)
        x = F.dropout(x, training=self.training)
        x = self.conv2(x, edge_index, weight)
        return F.log_softmax(x, dim=1)


X = pd.read_csv('./input/app_api_300.csv', header=0, index_col='Unnamed: 0')
y = X.pop('Label')
label_encoding = preprocessing.LabelEncoder().fit_transform(y)
x = X.to_numpy()

dataset = from_networkx(load_data_2_create_graph('./input/m1.csv'))
dataset.x = torch.from_numpy(x).float()
dataset.y = torch.from_numpy(label_encoding)
dataset.num_classes = 5

print(dataset)

# print(dataset.edge_index.max().item())
# 1120579 removed
# 2113552 not yet removed

# train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(x, [500, 300, 700])

# train_mask = [i in train_dataset.indices for i in range(len(x))]
# val_mask = [i in val_dataset.indices for i in range(len(x))]
# test_mask = [i in test_dataset.indices for i in range(len(x))]

# dataset.train_mask = torch.asarray(train_mask)
# dataset.val_mask = torch.asarray(val_mask)
# dataset.test_mask = torch.asarray(test_mask)


# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# model = GCN().to(device)
# optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)
# # loss_function = nn.CrossEntropyLoss()

# data = dataset.to(device)

# model.train()
# for epoch in range(100):
#     optimizer.zero_grad()
#     out = model(data)
#     loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])
#     # loss = loss_function(out[data.train_mask], data.y[data.train_mask])
#     loss.backward()
#     optimizer.step()

#     model.eval()
#     pred = model(data).argmax(dim=1)
#     correct = (pred[data.val_mask] == data.y[data.val_mask]).sum()
#     acc = int(correct) / int(data.val_mask.sum())
#     print(f'Epoch: {epoch}, Accuracy: {acc:.4f}')

# model.eval()
# pred = model(data).argmax(dim=1)
# correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()
# acc = int(correct) / int(data.test_mask.sum())
# print(f'Accuracy: {acc:.4f}')


# # Saving
# torch.save(model.state_dict(), './output/model_300.pth')
# with open('./output/encoder.pkl', 'wb') as f:
#     pickle.dump(label_encoding, f)
