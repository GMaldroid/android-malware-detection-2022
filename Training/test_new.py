from collections import Counter
import pickle
import pandas as pd
import numpy as np
import torch
from torch.utils.data import random_split
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, GraphConv
import torch.nn.functional as F


num_top = 100


def add_new_nodes(dataset, new_nodes, new_node_label_encodes):
    X_train = pd.read_csv(f'../JsonFilesToMatrix/output/top_{num_top}_APIs/app_api_{num_top}_APIs_train.csv', header=0, index_col='Unnamed: 0')
    y_train = X_train.pop('Label')
    x_train = X_train.to_numpy()

    number_of_new_nodes = new_nodes.shape[0]
    edge_weight = torch.tensor(new_nodes @ x_train.T)
    max_elements, max_idxs = torch.max(edge_weight, 1)
    edge_weight = torch.div(edge_weight, max_elements.reshape(-1,1) + 0.001)
    edge_weight = edge_weight.flatten()

    new_node_index_start = dataset.edge_index.max().item() + 1
    index_0 = []
    index_1 = []
    for i in range(number_of_new_nodes):
        index_0.extend([new_node_index_start + i] * new_node_index_start)
        index_1.extend(list(range(0, new_node_index_start)))

    edge_index = torch.tensor([index_0, index_1])

    # add new val nodes to graph dataset
    dataset.x = torch.cat([dataset.x, torch.from_numpy(new_nodes).float()], dim=0)
    dataset.edge_index = torch.cat([dataset.edge_index, edge_index], dim=1)
    dataset.weight = torch.cat([dataset.weight, edge_weight], dim=-1)
    dataset.y = torch.cat([dataset.y, new_node_label_encodes], dim=-1)
    dataset.train_mask = torch.cat([dataset.train_mask, torch.tensor([False] * number_of_new_nodes)], dim=-1)
    dataset.val_mask = torch.cat([dataset.val_mask, torch.tensor([True] * number_of_new_nodes)], dim=-1)
    dataset.test_mask = torch.cat([dataset.test_mask, torch.tensor([False] * number_of_new_nodes)], dim=-1)

    return dataset


dataset = torch.load(f'./output/dataset_{num_top}.pt')

X = pd.read_csv(f'../JsonFilesToMatrix/output/top_{num_top}_APIs/app_api_{num_top}_APIs_test.csv', header=0, index_col='Unnamed: 0')
x_val_index = [i for i in range(0, 200)] + [i for i in range(800, 1000)] + [i for i in range(1600, 1800)] + [i for i in range(2400, 2600)] + \
    [i for i in range(3200, 3400)]
x_val = X.iloc[x_val_index]
x_test = X.drop(x_val_index)

y_val = x_val.pop('Label')
y_test = x_test.pop('Label')

with open(f'./output/encoder_{num_top}.pkl', 'rb') as fp:
    encoder = pickle.load(fp)

val_label_encode = torch.from_numpy(encoder.transform(y_val))
test_label_encode = torch.from_numpy(encoder.transform(y_test))
del y_test, y_val

x_val = x_val.to_numpy()
x_test = x_test.to_numpy()


# Train with x_val
dataset = add_new_nodes(dataset, x_val, val_label_encode)
# y_train_list = dataset.y[dataset.train_mask].tolist()
# y_val_list = dataset.y[dataset.val_mask].tolist()
# y_test_list = dataset.y[dataset.test_mask].tolist()

# print(f'Counter y_train label: {Counter(y_train_list)}')
# print(f'Counter y_val label: {Counter(y_val_list)}')
# print(f'Counter y_test label: {Counter(y_test_list)}')


class GCN(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = GraphConv(-1, 32, aggr='mean', normalize=True)
        self.conv2 = GraphConv(-1, 5, aggr='mean', normalize=True)

    def forward(self, data):
        x, edge_index, weight = data.x, data.edge_index, data.weight
        x = self.conv1(x, edge_index, weight)
        x = F.relu(x)
        x = F.dropout(x, training=self.training)
        x = self.conv2(x, edge_index)
        return F.log_softmax(x, dim=1)


epochs = 1000
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = GCN().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)
model.load_state_dict(torch.load(f"./output/model_{num_top}.pth"))

model.train()
for epoch in range(epochs):
    optimizer.zero_grad()
    out = model(dataset)
    loss = F.nll_loss(out[dataset.train_mask], dataset.y[dataset.train_mask])
    loss.backward()
    optimizer.step()

    model.eval()
    pred = model(dataset).argmax(dim=1)
    correct = (pred[dataset.val_mask] == dataset.y[dataset.val_mask]).sum()
    acc = int(correct) / int(dataset.val_mask.sum())
    
    print(f'Epoch: {epoch}, Accuracy: {acc:.4f}')
    # if epoch % 10 == 0:
    #     print(f'Epoch: {epoch}, Accuracy: {acc:.4f}')
    if epoch % 100 == 0:
        torch.save(model.state_dict(), f'./output/model_{num_top}_retrain_epoch_{epoch}.pth')


# model.eval()
# pred = model(dataset).argmax(dim=1)
# correct = (pred[dataset.test_mask] == dataset.y[dataset.test_mask]).sum()
# acc = int(correct) / int(dataset.test_mask.sum())
# print(f'Accuracy: {acc:.4f}')

# model.eval()
# pred = model(dataset).argmax(dim=1)
# correct = (pred[new_node_index_start:] == test_label_encode).sum()
# acc = int(correct) / test_label_encode.shape[0]
# print(f'Accuracy: {acc:.4f}')

# # result = torch.cat([pred[new_node_index_start:].reshape(1, -1), test_label_encode.reshape(1, -1)], dim=0)
# # result_np = result.numpy()
# # result_df = pd.DataFrame(result_np)
# # result_df.to_csv('./output/2000_4000_prediction.csv')
