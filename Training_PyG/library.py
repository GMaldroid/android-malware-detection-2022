import numpy as np
import pandas as pd
import torch
from torch_geometric.data import Data
from torch_geometric.loader import NeighborLoader
from torch.utils.data import random_split


def create_edge_index(node_number):
    index_0 = []
    index_1 = []

    for i in range(node_number):
        index_0.extend([i] * (i + 1))
        index_1.extend(list(range(0, i + 1)))

    return torch.tensor([index_0, index_1])


def create_edge(meta_path):
    adj_mtrx = pd.read_csv(meta_path, index_col=None, header=None)
    node_number = adj_mtrx.shape[0]

    # edge_index
    _edge_index = create_edge_index(node_number)

    # edge_weight
    adj_mtrx = adj_mtrx.to_numpy()
    adj_mtrx = adj_mtrx / (np.amax(adj_mtrx, axis=1) + 0.001)
    adj_mtrx = np.tril(adj_mtrx)

    _edge_weight = []
    for i in range(node_number):
        _edge_weight.extend(adj_mtrx[i, :i + 1])

    return _edge_index, torch.tensor(_edge_weight, dtype=torch.float)


def get_dataset(meta_path, x, y, num_top=400):
    edge_index, edge_weight = create_edge(
        f'../JsonFilesToMatrix/output/top_{num_top}_APIs/{meta_path}_{num_top}_APIs.csv'
    )
    dataset = Data(x=x, y=y, edge_index=edge_index, edge_weight=edge_weight)

    # Saving
    torch.save(dataset, f'./output/full_dataset_{num_top}_{meta_path}.pt')

    torch.manual_seed(42)
    train_dataset, test_dataset = random_split(x, [2400, 600], torch.Generator().manual_seed(42))
    train_mask = [i in train_dataset.indices for i in range(len(dataset.x))]
    test_mask = [i in test_dataset.indices for i in range(len(dataset.x))]
    dataset.test_mask = torch.asarray(test_mask)

    torch.manual_seed(42)
    train_dataset = dataset.subgraph(torch.asarray(train_mask))

    torch.manual_seed(42)
    sub_train_dataset, sub_val_dataset, sub_test_dataset = random_split(
        train_dataset.x, [1440, 384, 576], torch.Generator().manual_seed(42)
    )
    sub_train_mask = [i in sub_train_dataset.indices for i in range(len(train_dataset.x))]
    sub_val_mask = [i in sub_val_dataset.indices for i in range(len(train_dataset.x))]
    sub_test_mask = [i in sub_test_dataset.indices for i in range(len(train_dataset.x))]
    train_dataset.train_mask = torch.asarray(sub_train_mask)
    train_dataset.val_mask = torch.asarray(sub_val_mask)
    train_dataset.test_mask = torch.asarray(sub_test_mask)

    return dataset, train_dataset


def split_batch(dataset, num_neighbors=[10, 20], batch_size=50):
    torch.manual_seed(42)
    train_loader = [data for data in NeighborLoader(data=dataset, input_nodes=dataset.train_mask, num_neighbors=num_neighbors, batch_size=batch_size, directed=True, shuffle=False, num_workers=1)]
    val_loader = [data for data in NeighborLoader(data=dataset, input_nodes=dataset.val_mask, num_neighbors=num_neighbors, batch_size=batch_size, directed=True, shuffle=False, num_workers=1)]
    test_loader = [data for data in NeighborLoader(data=dataset, input_nodes=dataset.test_mask, num_neighbors=num_neighbors, batch_size=batch_size, directed=True, shuffle=False, num_workers=1)]

    return train_loader, val_loader, test_loader
