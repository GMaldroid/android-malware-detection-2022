from os import path
from collections import Counter
import pickle
import numpy as np
# import matplotlib.pyplot as plt
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import random_split
import torch.nn.functional as F
from torch_geometric.nn import GraphConv, SAGEConv
from torch_geometric.data import Data

from sklearn import preprocessing


num_top = 400
epochs = 50
layer_nodes = [64, 32]
lr = 0.003
dropout = 0.3


def create_edge_index(node_number):
    index_0 = []
    index_1 = []
    
    for i in range(node_number):
        index_0.extend([i]*(i+1))
        index_1.extend(list(range(0, i+1)))
    
    return torch.tensor([index_0, index_1])


def create_edge(meta_path):
    adj_matr = pd.read_csv(meta_path, index_col=None, header=None)
    node_number = adj_matr.shape[0]
    
    #edge_index
    edge_index = create_edge_index(node_number)
    
    #weight
    adj_matr = adj_matr.to_numpy()
    adj_matr = adj_matr / (np.amax(adj_matr, axis=1) + 0.001)
    adj_matr = np.tril(adj_matr)

    weight = []
    for i in range(node_number):
        weight.extend(adj_matr[i, :i+1])
    return edge_index, torch.tensor(weight, dtype=torch.float)


class GraphSageModel(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = GraphConv(-1, layer_nodes[0], aggr='mean', normalize=True)
        self.conv2 = GraphConv(-1, layer_nodes[1], aggr='mean', normalize=True)
        self.dense = nn.Linear(layer_nodes[1], 5)

    def forward(self, data):
        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_weight
        x = self.conv1(x, edge_index, edge_weight)
        x = F.relu(x)
        x = F.dropout(x, p=dropout, training=self.training)
        x = self.conv2(x, edge_index, edge_weight)
        x = F.dropout(x, p=dropout, training=self.training)
        x = self.dense(x)
        return F.log_softmax(x, dim=1)


if path.isfile(f'./output/dataset_{num_top}.pt'):
    dataset = torch.load(f'./output/full_dataset_{num_top}.pt')
else:
    X = pd.read_csv(f'../JsonFilesToMatrix/output/top_{num_top}_APIs/app_api_{num_top}_APIs_train.csv', header=0, index_col='Unnamed: 0')
    y = X.pop('Label')
    label_encoder = preprocessing.LabelEncoder().fit(y)
    with open(f'./output/encoder_{num_top}.pkl', 'wb') as f:
        pickle.dump(label_encoder, f)

    x = torch.from_numpy(X.to_numpy()).float()

    edge_index, edge_weight = create_edge(f'../JsonFilesToMatrix/output/top_{num_top}_APIs/m1_{num_top}_APIs.csv')
    dataset = Data(x=x, y=torch.from_numpy(label_encoder.transform(y)), edge_index=edge_index, edge_weight=edge_weight)
    torch.save(dataset, f'./output/full_dataset_{num_top}.pt')


train_dataset, test_dataset = random_split(x, [2400, 600], torch.Generator().manual_seed(42)) # type: ignore
train_mask = [i in train_dataset.indices for i in range(len(dataset.x))]
test_mask = [i in test_dataset.indices for i in range(len(dataset.x))]
dataset.test_mask = torch.asarray(test_mask)

train_dataset = dataset.subgraph(torch.asarray(train_mask))

sub_train_dataset, sub_val_dataset, sub_test_dataset = random_split(train_dataset.x, [1440, 384, 576], torch.Generator().manual_seed(42)) # type: ignore
sub_train_mask = [i in sub_train_dataset.indices for i in range(len(train_dataset.x))]
sub_val_mask = [i in sub_val_dataset.indices for i in range(len(train_dataset.x))]
sub_test_mask = [i in sub_test_dataset.indices for i in range(len(train_dataset.x))]
train_dataset.train_mask = torch.asarray(sub_train_mask)
train_dataset.val_mask = torch.asarray(sub_val_mask)
train_dataset.test_mask = torch.asarray(sub_val_mask)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = GraphSageModel().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)
test_dataset = dataset
dataset = train_dataset

if path.isfile(f'./output/model_{num_top}.pth'):
    model.load_state_dict(torch.load(f"./output/model_{num_top}.pth"))
# Training
model.train()
for epoch in range(epochs):
    optimizer.zero_grad()
    out = model(dataset)
    loss = F.nll_loss(out[dataset.train_mask], dataset.y[dataset.train_mask])
    loss.backward()
    optimizer.step()

    model.eval()
    pred = model(dataset).argmax(dim=1)
    correct = (pred[dataset.val_mask] == dataset.y[dataset.val_mask]).sum()
    acc = int(correct) / int(dataset.val_mask.sum())
    print(f'Epoch: {epoch}, Accuracy: {acc:.4f}')

# Saving
torch.save(model.state_dict(), f'./output/model_{num_top}.pth')

# Testing
model.eval()
pred = model(dataset).argmax(dim=1)
correct = (pred[dataset.test_mask] == dataset.y[dataset.test_mask]).sum()
acc = int(correct) / int(dataset.test_mask.sum())
print(f'Accuracy test subgraph: {acc:.4f}')

model.eval()
pred = model(test_dataset).argmax(dim=1)
correct = (pred[test_dataset.test_mask] == test_dataset.y[test_dataset.test_mask]).sum()
acc = int(correct) / int(test_dataset.test_mask.sum())
print(f'Accuracy test fullgraph: {acc:.4f}')
