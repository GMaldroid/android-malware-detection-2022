import pickle
# import matplotlib.pyplot as plt
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GraphConv
from torch_geometric.loader import NeighborLoader

from sklearn import preprocessing
from library import get_dataset, split_batch


num_top = 400
epochs = 50
layer_nodes = [64, 32]
lr = 0.001
dropout = 0.3
batch_size = 50
num_neighbors = [10, 20]


class GraphSageModel(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1_m1 = GraphConv(-1, layer_nodes[0], aggr='mean', normalize=True)
        self.conv2_m1 = GraphConv(-1, layer_nodes[1], aggr='mean', normalize=True)
        self.conv1_m2 = GraphConv(-1, layer_nodes[0], aggr='mean', normalize=True)
        self.conv2_m2 = GraphConv(-1, layer_nodes[1], aggr='mean', normalize=True)
        self.conv1_m3 = GraphConv(-1, layer_nodes[0], aggr='mean', normalize=True)
        self.conv2_m3 = GraphConv(-1, layer_nodes[1], aggr='mean', normalize=True)
        self.conv1_m4 = GraphConv(-1, layer_nodes[0], aggr='mean', normalize=True)
        self.conv2_m4 = GraphConv(-1, layer_nodes[1], aggr='mean', normalize=True)
        self.dense = nn.Linear(layer_nodes[1] * 4, 5)

    def forward(self, data_m1, data_m2, data_m3, data_m4):
        x_m1 = self.conv1_m1(data_m1.x, data_m1.edge_index, data_m1.edge_weight)
        x_m1 = F.relu(x_m1)
        x_m1 = F.dropout(x_m1, p=dropout, training=self.training)
        x_m1 = self.conv2_m1(x_m1, data_m1.edge_index, data_m1.edge_weight)
        x_m1 = F.dropout(x_m1, p=dropout, training=self.training)

        x_m2 = self.conv1_m2(data_m2.x, data_m2.edge_index, data_m2.edge_weight)
        x_m2 = F.relu(x_m2)
        x_m2 = F.dropout(x_m2, p=dropout, training=self.training)
        x_m2 = self.conv2_m2(x_m2, data_m2.edge_index, data_m2.edge_weight)
        x_m2 = F.dropout(x_m2, p=dropout, training=self.training)

        x_m3 = self.conv1_m3(data_m3.x, data_m3.edge_index, data_m3.edge_weight)
        x_m3 = F.relu(x_m3)
        x_m3 = F.dropout(x_m3, p=dropout, training=self.training)
        x_m3 = self.conv2_m3(x_m3, data_m3.edge_index, data_m3.edge_weight)
        x_m3 = F.dropout(x_m3, p=dropout, training=self.training)

        x_m4 = self.conv1_m4(data_m4.x, data_m4.edge_index, data_m4.edge_weight)
        x_m4 = F.relu(x_m4)
        x_m4 = F.dropout(x_m4, p=dropout, training=self.training)
        x_m4 = self.conv2_m4(x_m4, data_m4.edge_index, data_m4.edge_weight)
        x_m4 = F.dropout(x_m4, p=dropout, training=self.training)

        _x = torch.cat((x_m1, x_m2, x_m3, x_m4), dim=1)
        _x = self.dense(_x)
        return F.log_softmax(_x, dim=1)


X = pd.read_csv(f'../JsonFilesToMatrix/output/top_{num_top}_APIs/app_api_{num_top}_APIs_train.csv', header=0,
                index_col='Unnamed: 0')
Y = X.pop('Label')
label_encoder = preprocessing.LabelEncoder().fit(Y)
pickle.dump(label_encoder, open(f'./output/encoder_{num_top}.pkl', 'wb'))

x = torch.from_numpy(X.to_numpy()).float()
y = torch.from_numpy(label_encoder.transform(Y))

dataset_m1, train_dataset_m1 = get_dataset('m1', x, y, num_top)
dataset_m2, train_dataset_m2 = get_dataset('m2', x, y, num_top)
dataset_m3, train_dataset_m3 = get_dataset('m3', x, y, num_top)
dataset_m4, train_dataset_m4 = get_dataset('m4', x, y, num_top)

train_loader_m1, val_loader_m1, test_loader_m1 = split_batch(train_dataset_m1, num_neighbors, batch_size)
train_loader_m2, val_loader_m2, test_loader_m2 = split_batch(train_dataset_m2, num_neighbors, batch_size)
train_loader_m3, val_loader_m3, test_loader_m3 = split_batch(train_dataset_m3, num_neighbors, batch_size)
train_loader_m4, val_loader_m4, test_loader_m4 = split_batch(train_dataset_m4, num_neighbors, batch_size)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = GraphSageModel().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)

# Training
train_loss_history = []
train_acc_history = []
val_loss_history = []
val_acc_history = []

model.train()
for epoch in range(epochs):
    train_loss = 0
    train_acc = 0
    train_node_total = 0
    for i in range(len(train_loader_m1)):
        train_batch_node = train_loader_m1[i].x.shape[0]
        train_node_total += train_batch_node

        data_m1 = train_loader_m1[i].to(device)
        data_m2 = train_loader_m2[i].to(device)
        data_m3 = train_loader_m3[i].to(device)
        data_m4 = train_loader_m4[i].to(device)

        optimizer.zero_grad()
        out = model(data_m1, data_m2, data_m3, data_m4)
        pred = out.argmax(dim=1)
        correct = (pred[data_m1.train_mask] == data_m1.y[data_m1.train_mask]).sum()
        acc = int(correct) / int(data_m1.train_mask.sum())
        loss = F.nll_loss(out[data_m1.train_mask], data_m1.y[data_m1.train_mask])
        train_acc += acc * train_batch_node
        train_loss += loss * train_batch_node
        loss.backward()
        optimizer.step()

    train_acc_history.append(train_acc / train_node_total)
    train_loss_history.append((train_loss / train_node_total).cpu().detach().numpy().item())

    val_loss = 0
    val_acc = 0
    val_node_total = 0
    model.eval()
    for i in range(len(val_loader_m1)):
        val_batch_node = train_loader_m1[i].x.shape[0]
        val_node_total += val_batch_node

        data_m1 = val_loader_m1[i].to(device)
        data_m2 = val_loader_m2[i].to(device)
        data_m3 = val_loader_m3[i].to(device)
        data_m4 = val_loader_m4[i].to(device)

        out = model(data_m1, data_m2, data_m3, data_m4)
        pred = out.argmax(dim=1)
        correct = (pred[data_m1.val_mask] == data_m1.y[data_m1.val_mask]).sum()
        acc = int(correct) / int(data_m1.val_mask.sum())
        loss = F.nll_loss(out[data_m1.val_mask], data_m1.y[data_m1.val_mask])
        val_acc += acc * val_batch_node
        val_loss += loss * val_batch_node

    val_acc_history.append(val_acc / val_node_total)
    val_loss_history.append((val_loss / val_node_total).cpu().detach().numpy().item())

    print(
        f'Epoch {epoch}: train_acc: {train_acc / train_node_total:.4f}, train_loss: {train_loss / train_node_total:.4f}, val_acc: {val_acc / val_node_total:.4f}, val_loss: {val_loss / val_node_total:.4f}')

# Saving
torch.save(model.state_dict(), f'./output/model_{num_top}.pth')

# Testing with subgraph
test_subgraph_acc = 0
test_subgraph_node_total = 0
model.eval()

for i in range(len(test_loader_m1)):
    test_batch_node = test_loader_m1[i].x.shape[0]
    test_subgraph_node_total += test_batch_node

    data_m1 = test_loader_m1[i].to(device)
    data_m2 = test_loader_m2[i].to(device)
    data_m3 = test_loader_m3[i].to(device)
    data_m4 = test_loader_m4[i].to(device)

    pred = model(data_m1, data_m2, data_m3, data_m4).argmax(dim=1)
    correct = (pred[data_m1.test_mask] == data_m1.y[data_m1.test_mask]).sum()
    acc = int(correct) / int(data_m1.test_mask.sum())
    test_subgraph_acc += acc * test_batch_node

print(f'Accuracy of test subgraph: {test_subgraph_acc/test_subgraph_node_total:.4f}')

# Testing with fullgraoh
torch.manual_seed(42)
test_full_loader_m1 = [data for data in  NeighborLoader(data=dataset_m1, input_nodes=dataset_m1.test_mask, num_neighbors=num_neighbors, batch_size=batch_size, directed=True, shuffle=False, num_workers=1)]

torch.manual_seed(42)
test_full_loader_m2 = [data for data in  NeighborLoader(data=dataset_m2, input_nodes=dataset_m2.test_mask, num_neighbors=num_neighbors, batch_size=batch_size, directed=True, shuffle=False, num_workers=1)]

torch.manual_seed(42)
test_full_loader_m3 = [data for data in  NeighborLoader(data=dataset_m3, input_nodes=dataset_m3.test_mask, num_neighbors=num_neighbors, batch_size=batch_size, directed=True, shuffle=False, num_workers=1)]

torch.manual_seed(42)
test_full_loader_m4 = [data for data in  NeighborLoader(data=dataset_m4, input_nodes=dataset_m4.test_mask, num_neighbors=num_neighbors, batch_size=batch_size, directed=True, shuffle=False, num_workers=1)]

test_fullgraph_acc = 0
test_fullgraph_node_total = 0
model.eval()

for i in range(len(test_full_loader_m1)):
    test_batch_node = test_full_loader_m1[i].x.shape[0]
    test_fullgraph_node_total += test_batch_node

    data_m1 = test_full_loader_m1[i].to(device)
    data_m2 = test_full_loader_m2[i].to(device)
    data_m3 = test_full_loader_m3[i].to(device)
    data_m4 = test_full_loader_m4[i].to(device)

    pred = model(data_m1, data_m2, data_m3, data_m4).argmax(dim=1)
    correct = (pred[data_m1.test_mask] == data_m1.y[data_m1.test_mask]).sum()
    acc = int(correct) / int(data_m1.test_mask.sum())
    test_fullgraph_acc += acc * test_batch_node

print(f'Accuracy of test subgraph: {test_fullgraph_acc/test_fullgraph_node_total:.4f}')
